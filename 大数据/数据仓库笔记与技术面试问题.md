

# 基础部分 面试基本情况

> ## 面试题：自我介绍

# 第一部分 大数据概念理解

## 第一节、关系型数据库

1. 什么是缓慢变化维（SCD）？如何处理？**
   - **回答要点**：维度表中某些属性会随时间变化（如客户地址）。
     - **类型1**：直接覆盖旧值（不保留历史）。
     - **类型2**：添加新行，标记生效时间（保留历史）。
     - **类型3**：添加新列记录旧值（仅保留有限历史）。
2. **什么是事务？ACID 特性是什么？**
   - **回答要点**：事务是数据库操作的原子单元。ACID 特性：
     - **原子性（Atomicity）**：事务要么全部完成，要么全部回滚。
     - **一致性（Consistency）**：事务前后数据库状态一致。
     - **隔离性（Isolation）**：并发事务互不干扰。
     - **持久性（Durability）**：事务提交后数据永久保存。
     - - - 
     - 

## 第二节、数据仓库

### 一、如何理解数据仓库

> "A Data Warehouse is a <u>**subject-oriented**</u>, <u>**integrated**</u>,  <u>**non-volatile**</u> and <u>**time-variant**</u>  collection of data , designed to support management decision-making and data analysis. support of management's decision-making process."
>
> ​								--“数据仓库之父”的比尔·恩门（Bill Inmon）
>
> 数据仓库是一个面向主题的、集成的、相对稳定的、随时间变化的数据集合，用于支持管理决策.
>

数据仓库是为了分析数据而设计的。说人话就是用于支持管理决策

数据库为了捕获、存储而设计的。

### 二、数据库和数据仓库的区别（OLTP 和OL AP）的区别？

- **OLTP** 用于支持日常业务操作，特点是高并发、低延迟、频繁更新。

- **OLAP** 用于支持数据分析和决策制定，特点是复杂查询、大规模数据聚合、历史数据分析。

  > 面试题：数据库和数据仓库的区别？
  >
  > 答：
  >
  > | **特性**     | **OLTP**                     | **OLAP**                             |
  > | :----------- | :--------------------------- | :----------------------------------- |
  > | **目标**     | 支持日常业务操作             | 支持数据分析和决策制定               |
  > | **数据量**   | 数据量较小，存储当前数据     | 数据量较大，存储历史数据             |
  > | **数据更新** | 频繁更新（增删改查操作）     | 很少更新，主要用于查询               |
  > | **数据结构** | 规范化设计（减少冗余）       | 非规范化设计（如星型模型、雪花模型） |
  > | **查询类型** | 简单的事务性查询             | 复杂的分析性查询                     |
  > | **性能要求** | 高并发、低延迟               | 高吞吐量、支持复杂计算               |
  > | **用户**     | 业务操作人员（如店员、客服） | 数据分析师、管理层                   |

  

- 

### 三、数仓理论

#### 1、传统离线数仓分层

##### 1、**ODS**（operation data store） 贴源层

贴源层是用于<u>**存放从源系统提取的原始数据**</u>。

这些数据在贴源层中<u>**不做任何处理，**</u>保持与源系统相同的结构和格式

项目：数据来源：深交所的供应商，主要有1、国家专利局数据（发明专利、实用新型和外观设计，法律上的数据），2、上市公司的基础数据和财务数据，3、国内高校科研院所基础数据，其他零零散散的一些。

##### 2、DW （data ware house）数仓层

​      1）数据细节层 DWD

​      2）数据中间层 DWM

​      3）数据服务层 DWS

##### 3、ADS（）数据应用层

**ADS层（Application Data Store）数据应用层**



> #### 面试题：为何要分层？
>
> 数据仓库分层的主要目的是<u>**实现数据的清晰管理**</u>、<u>**高效处理**</u>和<u>**灵活应用**</u>。
>
> 通过分层设计，可以明确数据的流向、提高数据质量、优化查询性能、支持多种分析场景，并提高系统的可维护性和扩展性。
>
> 例如，贴源层用于存储原始数据，数据仓库层用于整合和清洗数据，应用层用于支持业务分析和决策。
>
> 分层设计不仅降低了数据处理的复杂度，还为团队协作和数据治理提供了良好的基础
>
> 发明
>
> 

### 2、数据湖

3、数据中台、湖仓一体

4、lambda架构

数据源层

数据采集层

大数据平台层

数仓层

应用层

5、kappa架构



# 第二部分 主流框架技术栈

## 前提：现代理论基础-谷歌三篇论文

**1、Google File System（GFS）--2003年***

  HDFS的设计灵感，解决了存储方案。

- 分布式文件系统

- 采用主从架构，主节点管理元数据，分节点储存数据块。

2、**MapReduce: Simplified Data Processing on Large Clusters - 2004年**

  MapReduce成为Hadoop的核心计算框架，简化了数据处理。

3、**Bigtable: A Distributed Storage System for Structured Data - 2006年**

- 结构化数据的高效存储和访问'
- HBase'

## 第一节、Hadoop

**Hadoop** 是一个开源的<u>**分布式计算框架**</u>，专门用于存储和处理大规模数据集。它最初由 Doug Cutting 和 Mike Cafarella 开发，灵感来源于 Google 的三篇经典论文（GFS、MapReduce 和 Bigtable）。Hadoop 的核心设计目标是能够以低成本、高可靠性的方式处理海量数据。

### 1. **HDFS (Hadoop Distributed File System)**

- **功能**: HDFS 是 Hadoop 的分布式文件系统，<u>**用于存储大规模数据**</u>。

- **优点**:

  - **海量数据存储：**典型文件大小GB、TB级别，百万以上文件数量
  - **高容错性**: 数据被<u>**分割成块**</u>（默认 128MB），并在集群中<u>**多节点复制**</u>（默认 **3** 份）。
  - **高吞吐量**: 适合大规模离线批量处理
  - **构建成本低、安全可靠**

- **缺点**：

  - 不适合低延迟数据访问
  - 不适合大量小文件存储（远远小于128M)
  - 不支持并发写入
  - 不支持文件随机修改、仅支持追加写入

- **特点：**主从架构**:**

  - **NameNode**: 管理文件系统的<u>**元数据**</u>（如文件目录结构、块位置、文件属性）。

    - 备用节点
    - 心跳heartbeats,3秒检查一次dataNode的健康状态

  - **DataNode**: 存储实际数据块。

    - block副本存放策略：
      - 副本1：随机选择，优先空闲的DataNode节点
      - 副本2：放在不同的机架节点
      - 副本3：放在与第二个副本同一机架的不同节点
      - 副本N：随机选择

    ![image-20250204144358438](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250204144358438.png)



> 面试题：为什么dataNode为什么是128M？
>
> 答：块太大：寻址开销高，作业执行时间过长；
>
> ​		块太小：
>
> 



> 面试题：请简述一下HDFS读写操作流程？
>
> 答：一、写入文件操作流程：
>
> 1. **客户端发起写入请求**：
>    - 客户端调用 HDFS API（如 `FileSystem.create()`）发起文件写入请求。
> 2. **与 NameNode 通信**：
>    - 客户端向 NameNode 请求文件写入权限。
>    - NameNode 检查文件是否存在以及客户端是否有写入权限。
>    - 如果检查通过，NameNode 返回一组 DataNode 列表（用于存储数据块）。
> 3. **建立数据管道（Pipeline）**：
>    - 客户端根据 NameNode 返回的 DataNode 列表，建立一个数据写入管道。
>    - 管道通常由多个 DataNode 组成（默认 3 个副本）。
> 4. **数据分块写入**：
>    - 客户端将文件数据分成固定大小的块（默认 128MB 或 256MB）。
>    - 数据块通过管道依次写入各个 DataNode。
> 5. **确认写入成功**：
>    - 每个 DataNode 将数据写入本地磁盘，并向下一个 DataNode 发送数据。
>    - 最后一个 DataNode 确认写入成功后，依次向前传递确认信息。
>    - 客户端收到最终确认后，关闭文件写入流。
> 6. **更新元数据**：
>    - 客户端通知 NameNode 文件写入完成。
>    - NameNode 更新文件的元数据（如块的位置信息）。
>
> 二、读文件操作流程：
>
> 

### 2. **MapReduce**

- **功能**: MapReduce 是 Hadoop 的分布式计算框架，用于并行处理大规模数据集。
- **特点**:
  - **编程模型**: 将计算任务分为两个阶段：
    - **Map 阶段**: 对输入数据进行处理，生成<u>**键值对**</u>（key-value pairs）。
    - **Reduce 阶段**: 对 Map 的输出进行<u>**汇总和聚合**</u>。
  - **自动并行化**: 任务被分配到集群中的多个节点并行执行。
  - **容错性**: 如果某个节点失败，任务会自动重新分配到其他节点。

### 3. **YARN (Yet Another Resource Negotiator)**

- **功能**: YARN 是 Hadoop 的**资源管理**系统，负责集群资源的调度和管理。
- **特点**:
  - **解耦计算与资源管理**: YARN 将资源管理与任务调度分离，支持多种计算框架（如 MapReduce、Spark、Flink）。
  - **核心组件**:
    - **ResourceManager**: 全局资源管理器，负责分配集群资源。
    - **NodeManager**: 每个节点上的代理，负责管理单个节点的资源

## 第二节、Hive

### 一、定义：

- Hive 是建立在 Hadoop 之上的<u>**数据仓库工具，**</u>提供类似 SQL 的查询语言（HiveQL）。
- 主要功能是将 SQL 查询转换为 MapReduce 任务，在 Hadoop 上执行

> 面试题：**Hive 和传统数据库的区别是什么？**
>
> - **回答要点**：
>   - **存储**：Hive 数据存储在 HDFS，传统数据库用本地存储。
>   
>   - **计算**：Hive 用 MapReduce/Spark，传统数据库有专用引擎。
>   
>   - **延迟**：Hive 适合批处理，传统数据库支持实时查询。
>   
>     
>
> #### 面试题：hive和Hadoop之间的关系？
>
> 答：“Hive 和 Hadoop 是紧密相关但定位不同的技术。Hadoop 是一个分布式计算框架，提供数据存储（HDFS）和分布式计算（MapReduce）能力，适合大规模数据的存储和批处理任务。而 Hive 是建立在 Hadoop 之上的数据仓库工具，提供类似 SQL 的查询语言（HiveQL），将 SQL 查询转换为 MapReduce 任务在 Hadoop 上执行。
>
> ​	Hive 的主要优势是<u>**降低开发门槛**</u>、提高开发效率和支持复杂查询，适合数据仓库、批处理分析和复杂查询场景。
>
> ​	然而，Hive 的查询性能依赖于底层的 MapReduce 引擎，延迟较高，不适合实时查询。因此，Hive 和 Hadoop 是互补的关系，Hive 依赖于 Hadoop 提供的基础设施，同时简化了 Hadoop 的使用。“

**Hive 的分区和分桶有什么区别？**

- **分区（Partition）**：按目录划分数据（如 `dt=20231001`），加速按分区过滤的查询。
- **分桶（Bucket）**：按哈希值将数据分散到固定数量的文件中，优化 JOIN 和采样效率



## 第三节、spark

### 一、定义与理解

分布式计算框架

Spark Core API

Spark SQL

Struct Streaming

Spark ML lib

### 二、运行模式

|                             | 适用场景                                              | **特点**                                                     |
| --------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |
| 本地Local模式               | 开发、测试和调试                                      | 简单易用，无需集群环境。                                     |
| Standalone 模式（独立模式） | 中小规模集群，无需依赖外部集群管理器。                | 需要手动启动 Spark 集群（Master 和 Worker 节点）。 支持高可用性（HA）模式，可以通过 ZooKeeper 实现 Master 的故障恢复。 |
| **YARN 模式**               | 已经部署 Hadoop 的大数据集群                          | 在 Hadoop YARN（Yet Another Resource Negotiator）上运行 Spark，YARN 负责资源管理和任务调度。 |
| Mesos                       | 需要与其他框架（如 Hadoop、Kafka 等）共享资源的集群。 | 支持细粒度和粗粒度的资源调度。 适合多框架共享资源的场景。    |
| **Kubernetes 模式**         | 基于容器的云原生环境。                                | 支持动态资源分配和容器化部署。 适合云原生架构和微服务环境。  |





弹性分布式数据集



面试题

- Spark与Hadoop的区别？
- RDD、DataFrame和Dataset的区别？
- 如何优化Spark作业？
- spark为什么比mp快？
  - MR计算是基于磁盘的，Spark计算是基于内存的。
- 什么是宽依赖和窄依赖？
- 如何解决Spark中的数据倾斜问题？
- Spark如何实现容错？

## 四、flink



# 第三部分 SQL

经典题型：

hivesql

sparksql

# 第四部分 python



# 第五部分 算法


# 数据倾斜问题全面总结

## 一、数据倾斜的概念与表现

### 1. 什么是数据倾斜

java

```
// 数据倾斜的本质：数据分布不均匀
// 理想情况：每个节点处理的数据量大致相等
// 倾斜情况：少数节点处理绝大部分数据

// 示例：100个分区，其中5个分区包含90%的数据
```



### 2. 数据倾斜的表现

**系统表现：**

- 大部分Task快速完成，少数Task长时间运行
- 节点间负载严重不均衡
- 频繁出现OOM（内存溢出）错误
- 网络传输瓶颈

**监控指标：**

bash

```
# Spark监控
Stage持续时间异常长
个别Task处理数据量极大
GC时间异常增加

# Hadoop MapReduce
个别Map或Reduce任务执行时间过长
Reduce阶段卡在99%
```



## 二、数据倾斜的根本原因

### 1. 数据源本身倾斜

sql

```
-- 业务数据分布不均
-- 用户行为数据：少数热门商品占大部分点击
-- 日志数据：少数IP产生大量请求
-- 交易数据：大客户交易频繁

-- 示例：电商用户购买记录
用户ID     购买次数
user_001   150000  -- 倾斜Key
user_002   3
user_003   2
...
user_99999 1
```



### 2. Key设计不合理

sql

```
-- 问题Key设计
-- 1. 低基数Key：性别、状态码、布尔值
SELECT gender, COUNT(*) FROM users GROUP BY gender;
-- 只有2个Key，但数据量可能极大

-- 2. 空值或默认值集中
SELECT COALESCE(ip, 'unknown'), COUNT(*) 
FROM logs GROUP BY COALESCE(ip, 'unknown');

-- 3. 时间戳未分桶
SELECT DATE(timestamp), COUNT(*) 
FROM events GROUP BY DATE(timestamp);
-- 某天数据量特别大
```



### 3. Shuffle操作设计问题

scala

```
// Spark中容易引起倾斜的操作
val skewedRDD = originalRDD
  .groupByKey()        // 按Key分组
  .join(largeRDD)      // 大表Join
  .distinct()          // 去重
  .reduceByKey(_ + _)  // 按Key聚合

// 如果Key分布不均，就会产生数据倾斜
```



## 三、数据倾斜的检测方法

### 1. 数据探查分析

sql

```
-- 分析Key分布
SELECT key_column, COUNT(*) as cnt
FROM source_table
GROUP BY key_column
ORDER BY cnt DESC
LIMIT 20;

-- 检查数据倾斜度
SELECT 
  MAX(cnt) as max_count,
  MIN(cnt) as min_count, 
  AVG(cnt) as avg_count,
  MAX(cnt) / AVG(cnt) as skew_ratio
FROM (
  SELECT key_column, COUNT(*) as cnt
  FROM source_table
  GROUP BY key_column
) t;
```



### 2. 运行时监控

scala

```
// Spark监控倾斜
spark.sparkContext.setLogLevel("INFO")

// 查看Stage详情
// 关注：Task数据量差异、执行时间差异

// 自定义监控
val rdd = ... // 你的RDD
rdd.mapPartitionsWithIndex { (index, iterator) =>
  val count = iterator.size
  println(s"Partition $index has $count records")
  iterator
}.count()
```



### 3. 可视化分析工具

bash

```
# Spark UI分析
1. 查看Stages页面
2. 关注Task的Input Size / Shuffle Read Size
3. 识别数据量异常的Task

# Hadoop Counters
mapreduce.Job: 
  map output records
  reduce shuffle bytes
```



## 四、数据倾斜解决方案

### 1. 预处理方案

#### 数据采样与分桶

sql

```
-- 识别热点Key
WITH key_stats AS (
  SELECT 
    user_id,
    COUNT(*) as frequency,
    NTILE(100) OVER (ORDER BY COUNT(*) DESC) as percentile
  FROM user_behavior
  GROUP BY user_id
)
SELECT 
  percentile,
  COUNT(*) as key_count,
  SUM(frequency) as total_records
FROM key_stats
GROUP BY percentile
ORDER BY percentile;

-- 分桶处理
CREATE TABLE user_behavior_bucketed AS
SELECT 
  user_id,
  behavior_type,
  MD5(user_id) % 50 as bucket_id  -- 50个桶
FROM user_behavior;
```



#### 数据预处理

scala

```
// 分离热点数据
val dataRDD = ... // 原始数据

// 识别热点Key（假设前1%为热点）
val topKeys = dataRDD.map(_._1)
  .countByValue()
  .toSeq
  .sortBy(-_._2)
  .take(dataRDD.partitions.length / 100)

val hotKeys = topKeys.map(_._1).toSet
val broadcastHotKeys = spark.sparkContext.broadcast(hotKeys)

// 分离数据
val (hotData, normalData) = dataRDD.partition { case (key, value) =>
  broadcastHotKeys.value.contains(key)
}
```



### 2. 计算时优化方案

#### 增加Shuffle并行度

scala

```
// Spark设置更多分区
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.default.parallelism", "200")

// RDD操作
val rdd = originalRDD
  .repartition(200)  // 增加分区数
  .reduceByKey(_ + _)

// SQL Hint
sql("SELECT /*+ REPARTITION(200) */ * FROM table")
```



#### 两阶段聚合

scala

```
// 局部聚合 + 全局聚合
val result = dataRDD
  // 第一阶段：局部聚合，添加随机前缀
  .map { case (key, value) =>
    val randomPrefix = (Random.nextInt(10), key)
    (randomPrefix, value)
  }
  .reduceByKey(_ + _)  // 局部聚合
  
  // 第二阶段：全局聚合，去除前缀
  .map { case ((prefix, key), value) =>
    (key, value)
  }
  .reduceByKey(_ + _)  // 全局聚合
```



#### 广播Join优化

scala

```
// 小表广播，避免Shuffle
val largeTable = spark.table("large_fact_table")
val smallTable = spark.table("small_dim_table")

// 广播小表
val broadcastSmallTable = broadcast(smallTable)

val result = largeTable
  .join(broadcastSmallTable, "join_key")
  .select("large_table.*", "small_table.dim_column")
```



#### 倾斜Join特殊处理

scala

```
// 分离倾斜Key单独处理
// 1. 识别倾斜Key
val skewedKeys = Seq("hot_key_1", "hot_key_2", "hot_key_3")
val broadcastSkewedKeys = spark.sparkContext.broadcast(skewedKeys.toSet)

// 2. 分离数据
val leftTable = spark.table("left_table").rdd
val rightTable = spark.table("right_table").rdd

val (leftSkewed, leftNormal) = leftTable.map(row => (row.getString(0), row))
  .partition { case (key, value) =>
    broadcastSkewedKeys.value.contains(key)
  }

val (rightSkewed, rightNormal) = rightTable.map(row => (row.getString(0), row))
  .partition { case (key, value) =>
    broadcastSkewedKeys.value.contains(key)
  }

// 3. 分别处理
// 正常数据正常Join
val normalJoin = leftNormal.join(rightNormal)

// 倾斜数据添加随机后缀后Join
val leftSkewedExpanded = leftSkewed.flatMap { case (key, value) =>
  (0 until 10).map(i => (s"${key}_$i", value))
}

val rightSkewedExpanded = rightSkewed.flatMap { case (key, value) =>
  (0 until 10).map(i => (s"${key}_$i", value))
}

val skewedJoin = leftSkewedExpanded.join(rightSkewedExpanded)

// 4. 合并结果
val finalResult = normalJoin.union(skewedJoin)
```



### 3. SQL层优化方案

#### Hive数据倾斜优化

sql

```
-- 启用Group By负载均衡
SET hive.groupby.skewindata = true;

-- Join优化
SET hive.optimize.skewjoin = true;
SET hive.skewjoin.key = 100000; -- 超过10万条记录认为是倾斜

-- 使用MAPJOIN
SELECT /*+ MAPJOIN(small_table) */ 
       large_table.*, small_table.*
FROM large_table 
JOIN small_table ON large_table.id = small_table.id;
```



#### Spark SQL优化

scala

```
// 启用自适应查询执行
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.join.enabled", "true")

// 倾斜Join自动优化
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionFactor", "5")
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionThresholdInBytes", "256MB")

// 广播Join阈值
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")
```



## 五、不同场景的具体解决方案

### 1. Group By场景倾斜

sql

```
-- 原始SQL（可能倾斜）
SELECT user_id, COUNT(*) as cnt
FROM user_behavior
GROUP BY user_id;

-- 优化方案1：两阶段聚合
SELECT user_id, SUM(cnt) as total_cnt
FROM (
    SELECT 
        user_id, 
        FLOOR(RAND() * 10) as bucket,  -- 添加随机桶
        COUNT(*) as cnt
    FROM user_behavior
    GROUP BY user_id, bucket
) tmp
GROUP BY user_id;

-- 优化方案2：过滤热点单独处理
WITH hot_users AS (
    SELECT user_id
    FROM user_behavior
    GROUP BY user_id
    HAVING COUNT(*) > 100000  -- 定义热点阈值
),
normal_data AS (
    SELECT u.user_id, COUNT(*) as cnt
    FROM user_behavior u
    LEFT JOIN hot_users h ON u.user_id = h.user_id
    WHERE h.user_id IS NULL
    GROUP BY u.user_id
),
hot_data AS (
    SELECT user_id, COUNT(*) as cnt
    FROM user_behavior
    WHERE user_id IN (SELECT user_id FROM hot_users)
    GROUP BY user_id
)
SELECT * FROM normal_data
UNION ALL
SELECT * FROM hot_data;
```



### 2. Join场景倾斜

sql

```
-- 大表Join大表，某个Key数据量很大
SELECT a.*, b.*
FROM large_table_a a
JOIN large_table_b b ON a.join_key = b.join_key;

-- 优化：分离倾斜Key
WITH skewed_keys AS (
    SELECT join_key
    FROM large_table_a
    GROUP BY join_key
    HAVING COUNT(*) > 10000  -- 识别倾斜Key
),
a_skewed AS (
    SELECT a.*
    FROM large_table_a a
    JOIN skewed_keys s ON a.join_key = s.join_key
),
a_normal AS (
    SELECT a.*
    FROM large_table_a a
    LEFT JOIN skewed_keys s ON a.join_key = s.join_key
    WHERE s.join_key IS NULL
),
b_skewed AS (
    SELECT b.*
    FROM large_table_b b
    JOIN skewed_keys s ON b.join_key = s.join_key
),
b_normal AS (
    SELECT b.*
    FROM large_table_b b
    LEFT JOIN skewed_keys s ON b.join_key = s.join_key
    WHERE s.join_key IS NULL
),
-- 正常Key直接Join
normal_join AS (
    SELECT a.*, b.*
    FROM a_normal a
    JOIN b_normal b ON a.join_key = b.join_key
),
-- 倾斜Key添加随机后缀后Join
skewed_join AS (
    SELECT a.*, b.*
    FROM (
        SELECT *, 
               CONCAT(join_key, '_', FLOOR(RAND() * 10)) as join_key_suffix
        FROM a_skewed
    ) a
    JOIN (
        SELECT *,
               CONCAT(join_key, '_', suffix) as join_key_suffix
        FROM b_skewed
        LATERAL VIEW EXPLODE(ARRAY(0,1,2,3,4,5,6,7,8,9)) t AS suffix
    ) b ON a.join_key_suffix = b.join_key_suffix
)
SELECT * FROM normal_join
UNION ALL
SELECT * FROM skewed_join;
```



### 3. Distinct场景倾斜

sql

```
-- 原始去重（可能倾斜）
SELECT DISTINCT user_id
FROM user_behavior;

-- 优化：先局部去重，再全局去重
SELECT DISTINCT user_id
FROM (
    SELECT user_id
    FROM user_behavior
    GROUP BY user_id, FLOOR(RAND() * 20)  -- 先按用户+随机数分组
) tmp;
```



## 六、平台级解决方案

### 1. Spark Adaptive Query Execution

scala

```
// 启用AQE相关配置
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")

// AQE自动处理倾斜Join
spark.conf.set("spark.sql.adaptive.skew.join.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionFactor", "5")
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionThresholdInBytes", "256MB")
```



### 2. Flink数据倾斜处理

java

```
// Flink重新平衡数据
DataStream<String> input = ...;

// 使用重新平衡
DataStream<String> rebalanced = input.rebalance();

// 使用Rescale
DataStream<String> rescaled = input.rescale();

// 自定义Key选择器避免热点
DataStream<Tuple2<String, Integer>> keyed = input
    .keyBy(new KeySelector<String, String>() {
        @Override
        public String getKey(String value) {
            // 添加随机后缀分散热点
            return value + "_" + (int)(Math.random() * 10);
        }
    });
```



## 七、预防与监控体系

### 1. 数据质量监控

sql

```
-- 定期检查数据分布
CREATE TABLE data_skew_monitor AS
SELECT 
    table_name,
    partition_key,
    COUNT(*) as record_count,
    AVG(COUNT(*)) OVER () as avg_count,
    COUNT(*) / AVG(COUNT(*)) OVER () as skew_ratio
FROM source_tables
GROUP BY table_name, partition_key
HAVING skew_ratio > 10;  -- 倾斜阈值
```



### 2. 开发规范

java

```
// 代码规范建议
/**
 * 数据倾斜预防规范：
 * 1. 避免使用低基数字段作为Shuffle Key
 * 2. 对可能的热点Key进行预处理
 * 3. 合理设置Shuffle分区数
 * 4. 大表Join优先考虑广播Join
 * 5. 重要任务必须包含倾斜检测逻辑
 */
```



### 3. 自动化检测

python

```
# 倾斜检测脚本
def detect_skew(spark, table_name, key_column):
    """检测表的数据倾斜情况"""
    df = spark.table(table_name)
    
    skew_info = df.groupBy(key_column)\
        .count()\
        .select(
            F.max("count").alias("max_count"),
            F.min("count").alias("min_count"), 
            F.avg("count").alias("avg_count"),
            F.stddev("count").alias("stddev_count")
        )\
        .collect()[0]
    
    skew_ratio = skew_info["max_count"] / skew_info["avg_count"]
    return skew_ratio > 10  # 返回是否倾斜
```



## 八、总结

### 数据倾斜处理原则

1. **预防为主**：在数据建模阶段考虑均匀分布
2. **尽早识别**：通过监控和探查提前发现
3. **分层处理**：从数据源、计算、存储多层面解决
4. **权衡利弊**：解决方案可能带来额外开销，需要权衡