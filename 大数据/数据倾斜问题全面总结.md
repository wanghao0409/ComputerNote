# 数据倾斜问题全面总结

## 一、数据倾斜的概念与表现

### 1. 什么是数据倾斜

java

```
// 数据倾斜的本质：数据分布不均匀
// 理想情况：每个节点处理的数据量大致相等
// 倾斜情况：少数节点处理绝大部分数据

// 示例：100个分区，其中5个分区包含90%的数据
```



### 2. 数据倾斜的表现

**系统表现：**

- 大部分Task快速完成，少数Task长时间运行
- 节点间负载严重不均衡
- 频繁出现OOM（内存溢出）错误
- 网络传输瓶颈

**监控指标：**

bash

```
# Spark监控
Stage持续时间异常长
个别Task处理数据量极大
GC时间异常增加

# Hadoop MapReduce
个别Map或Reduce任务执行时间过长
Reduce阶段卡在99%
```



## 二、数据倾斜的根本原因

### 1. 数据源本身倾斜

sql

```
-- 业务数据分布不均
-- 用户行为数据：少数热门商品占大部分点击
-- 日志数据：少数IP产生大量请求
-- 交易数据：大客户交易频繁

-- 示例：电商用户购买记录
用户ID     购买次数
user_001   150000  -- 倾斜Key
user_002   3
user_003   2
...
user_99999 1
```



### 2. Key设计不合理

sql

```
-- 问题Key设计
-- 1. 低基数Key：性别、状态码、布尔值
SELECT gender, COUNT(*) FROM users GROUP BY gender;
-- 只有2个Key，但数据量可能极大

-- 2. 空值或默认值集中
SELECT COALESCE(ip, 'unknown'), COUNT(*) 
FROM logs GROUP BY COALESCE(ip, 'unknown');

-- 3. 时间戳未分桶
SELECT DATE(timestamp), COUNT(*) 
FROM events GROUP BY DATE(timestamp);
-- 某天数据量特别大
```



### 3. Shuffle操作设计问题

scala

```
// Spark中容易引起倾斜的操作
val skewedRDD = originalRDD
  .groupByKey()        // 按Key分组
  .join(largeRDD)      // 大表Join
  .distinct()          // 去重
  .reduceByKey(_ + _)  // 按Key聚合

// 如果Key分布不均，就会产生数据倾斜
```



## 三、数据倾斜的检测方法

### 1. 数据探查分析

sql

```
-- 分析Key分布
SELECT key_column, COUNT(*) as cnt
FROM source_table
GROUP BY key_column
ORDER BY cnt DESC
LIMIT 20;

-- 检查数据倾斜度
SELECT 
  MAX(cnt) as max_count,
  MIN(cnt) as min_count, 
  AVG(cnt) as avg_count,
  MAX(cnt) / AVG(cnt) as skew_ratio
FROM (
  SELECT key_column, COUNT(*) as cnt
  FROM source_table
  GROUP BY key_column
) t;
```



### 2. 运行时监控

scala

```
// Spark监控倾斜
spark.sparkContext.setLogLevel("INFO")

// 查看Stage详情
// 关注：Task数据量差异、执行时间差异

// 自定义监控
val rdd = ... // 你的RDD
rdd.mapPartitionsWithIndex { (index, iterator) =>
  val count = iterator.size
  println(s"Partition $index has $count records")
  iterator
}.count()
```



### 3. 可视化分析工具

bash

```
# Spark UI分析
1. 查看Stages页面
2. 关注Task的Input Size / Shuffle Read Size
3. 识别数据量异常的Task

# Hadoop Counters
mapreduce.Job: 
  map output records
  reduce shuffle bytes
```



## 四、数据倾斜解决方案

### 1. 预处理方案

#### 数据采样与分桶

sql

```
-- 识别热点Key
WITH key_stats AS (
  SELECT 
    user_id,
    COUNT(*) as frequency,
    NTILE(100) OVER (ORDER BY COUNT(*) DESC) as percentile
  FROM user_behavior
  GROUP BY user_id
)
SELECT 
  percentile,
  COUNT(*) as key_count,
  SUM(frequency) as total_records
FROM key_stats
GROUP BY percentile
ORDER BY percentile;

-- 分桶处理
CREATE TABLE user_behavior_bucketed AS
SELECT 
  user_id,
  behavior_type,
  MD5(user_id) % 50 as bucket_id  -- 50个桶
FROM user_behavior;
```



#### 数据预处理

scala

```
// 分离热点数据
val dataRDD = ... // 原始数据

// 识别热点Key（假设前1%为热点）
val topKeys = dataRDD.map(_._1)
  .countByValue()
  .toSeq
  .sortBy(-_._2)
  .take(dataRDD.partitions.length / 100)

val hotKeys = topKeys.map(_._1).toSet
val broadcastHotKeys = spark.sparkContext.broadcast(hotKeys)

// 分离数据
val (hotData, normalData) = dataRDD.partition { case (key, value) =>
  broadcastHotKeys.value.contains(key)
}
```



### 2. 计算时优化方案

#### 增加Shuffle并行度

scala

```
// Spark设置更多分区
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.default.parallelism", "200")

// RDD操作
val rdd = originalRDD
  .repartition(200)  // 增加分区数
  .reduceByKey(_ + _)

// SQL Hint
sql("SELECT /*+ REPARTITION(200) */ * FROM table")
```



#### 两阶段聚合

scala

```
// 局部聚合 + 全局聚合
val result = dataRDD
  // 第一阶段：局部聚合，添加随机前缀
  .map { case (key, value) =>
    val randomPrefix = (Random.nextInt(10), key)
    (randomPrefix, value)
  }
  .reduceByKey(_ + _)  // 局部聚合
  
  // 第二阶段：全局聚合，去除前缀
  .map { case ((prefix, key), value) =>
    (key, value)
  }
  .reduceByKey(_ + _)  // 全局聚合
```



#### 广播Join优化

scala

```
// 小表广播，避免Shuffle
val largeTable = spark.table("large_fact_table")
val smallTable = spark.table("small_dim_table")

// 广播小表
val broadcastSmallTable = broadcast(smallTable)

val result = largeTable
  .join(broadcastSmallTable, "join_key")
  .select("large_table.*", "small_table.dim_column")
```



#### 倾斜Join特殊处理

scala

```
// 分离倾斜Key单独处理
// 1. 识别倾斜Key
val skewedKeys = Seq("hot_key_1", "hot_key_2", "hot_key_3")
val broadcastSkewedKeys = spark.sparkContext.broadcast(skewedKeys.toSet)

// 2. 分离数据
val leftTable = spark.table("left_table").rdd
val rightTable = spark.table("right_table").rdd

val (leftSkewed, leftNormal) = leftTable.map(row => (row.getString(0), row))
  .partition { case (key, value) =>
    broadcastSkewedKeys.value.contains(key)
  }

val (rightSkewed, rightNormal) = rightTable.map(row => (row.getString(0), row))
  .partition { case (key, value) =>
    broadcastSkewedKeys.value.contains(key)
  }

// 3. 分别处理
// 正常数据正常Join
val normalJoin = leftNormal.join(rightNormal)

// 倾斜数据添加随机后缀后Join
val leftSkewedExpanded = leftSkewed.flatMap { case (key, value) =>
  (0 until 10).map(i => (s"${key}_$i", value))
}

val rightSkewedExpanded = rightSkewed.flatMap { case (key, value) =>
  (0 until 10).map(i => (s"${key}_$i", value))
}

val skewedJoin = leftSkewedExpanded.join(rightSkewedExpanded)

// 4. 合并结果
val finalResult = normalJoin.union(skewedJoin)
```



### 3. SQL层优化方案

#### Hive数据倾斜优化

sql

```
-- 启用Group By负载均衡
SET hive.groupby.skewindata = true;

-- Join优化
SET hive.optimize.skewjoin = true;
SET hive.skewjoin.key = 100000; -- 超过10万条记录认为是倾斜

-- 使用MAPJOIN
SELECT /*+ MAPJOIN(small_table) */ 
       large_table.*, small_table.*
FROM large_table 
JOIN small_table ON large_table.id = small_table.id;
```



#### Spark SQL优化

scala

```
// 启用自适应查询执行
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.join.enabled", "true")

// 倾斜Join自动优化
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionFactor", "5")
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionThresholdInBytes", "256MB")

// 广播Join阈值
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")
```



## 五、不同场景的具体解决方案

### 1. Group By场景倾斜

sql

```
-- 原始SQL（可能倾斜）
SELECT user_id, COUNT(*) as cnt
FROM user_behavior
GROUP BY user_id;

-- 优化方案1：两阶段聚合
SELECT user_id, SUM(cnt) as total_cnt
FROM (
    SELECT 
        user_id, 
        FLOOR(RAND() * 10) as bucket,  -- 添加随机桶
        COUNT(*) as cnt
    FROM user_behavior
    GROUP BY user_id, bucket
) tmp
GROUP BY user_id;

-- 优化方案2：过滤热点单独处理
WITH hot_users AS (
    SELECT user_id
    FROM user_behavior
    GROUP BY user_id
    HAVING COUNT(*) > 100000  -- 定义热点阈值
),
normal_data AS (
    SELECT u.user_id, COUNT(*) as cnt
    FROM user_behavior u
    LEFT JOIN hot_users h ON u.user_id = h.user_id
    WHERE h.user_id IS NULL
    GROUP BY u.user_id
),
hot_data AS (
    SELECT user_id, COUNT(*) as cnt
    FROM user_behavior
    WHERE user_id IN (SELECT user_id FROM hot_users)
    GROUP BY user_id
)
SELECT * FROM normal_data
UNION ALL
SELECT * FROM hot_data;
```



### 2. Join场景倾斜

sql

```
-- 大表Join大表，某个Key数据量很大
SELECT a.*, b.*
FROM large_table_a a
JOIN large_table_b b ON a.join_key = b.join_key;

-- 优化：分离倾斜Key
WITH skewed_keys AS (
    SELECT join_key
    FROM large_table_a
    GROUP BY join_key
    HAVING COUNT(*) > 10000  -- 识别倾斜Key
),
a_skewed AS (
    SELECT a.*
    FROM large_table_a a
    JOIN skewed_keys s ON a.join_key = s.join_key
),
a_normal AS (
    SELECT a.*
    FROM large_table_a a
    LEFT JOIN skewed_keys s ON a.join_key = s.join_key
    WHERE s.join_key IS NULL
),
b_skewed AS (
    SELECT b.*
    FROM large_table_b b
    JOIN skewed_keys s ON b.join_key = s.join_key
),
b_normal AS (
    SELECT b.*
    FROM large_table_b b
    LEFT JOIN skewed_keys s ON b.join_key = s.join_key
    WHERE s.join_key IS NULL
),
-- 正常Key直接Join
normal_join AS (
    SELECT a.*, b.*
    FROM a_normal a
    JOIN b_normal b ON a.join_key = b.join_key
),
-- 倾斜Key添加随机后缀后Join
skewed_join AS (
    SELECT a.*, b.*
    FROM (
        SELECT *, 
               CONCAT(join_key, '_', FLOOR(RAND() * 10)) as join_key_suffix
        FROM a_skewed
    ) a
    JOIN (
        SELECT *,
               CONCAT(join_key, '_', suffix) as join_key_suffix
        FROM b_skewed
        LATERAL VIEW EXPLODE(ARRAY(0,1,2,3,4,5,6,7,8,9)) t AS suffix
    ) b ON a.join_key_suffix = b.join_key_suffix
)
SELECT * FROM normal_join
UNION ALL
SELECT * FROM skewed_join;
```



### 3. Distinct场景倾斜

sql

```
-- 原始去重（可能倾斜）
SELECT DISTINCT user_id
FROM user_behavior;

-- 优化：先局部去重，再全局去重
SELECT DISTINCT user_id
FROM (
    SELECT user_id
    FROM user_behavior
    GROUP BY user_id, FLOOR(RAND() * 20)  -- 先按用户+随机数分组
) tmp;
```



## 六、平台级解决方案

### 1. Spark Adaptive Query Execution

scala

```
// 启用AQE相关配置
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")

// AQE自动处理倾斜Join
spark.conf.set("spark.sql.adaptive.skew.join.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionFactor", "5")
spark.conf.set("spark.sql.adaptive.skew.join.skewedPartitionThresholdInBytes", "256MB")
```



### 2. Flink数据倾斜处理

java

```
// Flink重新平衡数据
DataStream<String> input = ...;

// 使用重新平衡
DataStream<String> rebalanced = input.rebalance();

// 使用Rescale
DataStream<String> rescaled = input.rescale();

// 自定义Key选择器避免热点
DataStream<Tuple2<String, Integer>> keyed = input
    .keyBy(new KeySelector<String, String>() {
        @Override
        public String getKey(String value) {
            // 添加随机后缀分散热点
            return value + "_" + (int)(Math.random() * 10);
        }
    });
```



## 七、预防与监控体系

### 1. 数据质量监控

sql

```
-- 定期检查数据分布
CREATE TABLE data_skew_monitor AS
SELECT 
    table_name,
    partition_key,
    COUNT(*) as record_count,
    AVG(COUNT(*)) OVER () as avg_count,
    COUNT(*) / AVG(COUNT(*)) OVER () as skew_ratio
FROM source_tables
GROUP BY table_name, partition_key
HAVING skew_ratio > 10;  -- 倾斜阈值
```



### 2. 开发规范

java

```
// 代码规范建议
/**
 * 数据倾斜预防规范：
 * 1. 避免使用低基数字段作为Shuffle Key
 * 2. 对可能的热点Key进行预处理
 * 3. 合理设置Shuffle分区数
 * 4. 大表Join优先考虑广播Join
 * 5. 重要任务必须包含倾斜检测逻辑
 */
```



### 3. 自动化检测

python

```
# 倾斜检测脚本
def detect_skew(spark, table_name, key_column):
    """检测表的数据倾斜情况"""
    df = spark.table(table_name)
    
    skew_info = df.groupBy(key_column)\
        .count()\
        .select(
            F.max("count").alias("max_count"),
            F.min("count").alias("min_count"), 
            F.avg("count").alias("avg_count"),
            F.stddev("count").alias("stddev_count")
        )\
        .collect()[0]
    
    skew_ratio = skew_info["max_count"] / skew_info["avg_count"]
    return skew_ratio > 10  # 返回是否倾斜
```



## 八、总结

### 数据倾斜处理原则

1. **预防为主**：在数据建模阶段考虑均匀分布
2. **尽早识别**：通过监控和探查提前发现
3. **分层处理**：从数据源、计算、存储多层面解决
4. **权衡利弊**：解决方案可能带来额外开销，需要权衡